{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ACME-HappinessSurvey2020.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's do some exploratory data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "There are no missing data but the dataset is small. We should be careful with overfitting.\n",
    "\n",
    "Let's check the class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Y\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "55% are happy and 45% are unhappy. Seems pretty well balanced but this is misleading. More on that later.\n",
    "\n",
    "For now let's see the summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We see that\n",
    "\n",
    "- X1 has the largest mean (~4.33) and the lowest standard deviation (0.8), which means that satisfaction is high and variability is low. i.e. Most people are happy with little disagreement. X6 shows a similar case with a mean of ~ 4.25 and a std of ~0.81. Likewise for X4 with mean ~ 3.75 and std ~ 0.88.\n",
    "- X2 has the lowest mean (~2.53) with one of the highest std (~1.11) and a median of 3.0. Since median > mean, the distribution is left skewed meaning there are some very low scores (1s and 2s) that pull the mean below the median. Most reponses cluster around 3 but overall satisfaction is low.\n",
    "- X5 is also left skewed but the mean is higher (~3.65) with the highest std (~ 1.15). Most people are happy but there seems to be a large division between happy and unhappy people here.\n",
    "\n",
    "Let's check the distribution of the features by plotting them on bar plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_explanations = {\n",
    "    \"X1\": \"X1: Order delivered on time\",\n",
    "    \"X2\": \"X2: Order contents as expected\",\n",
    "    \"X3\": \"X3: Ordered everything wanted\",\n",
    "    \"X4\": \"X4: Paid a good price for order\",\n",
    "    \"X5\": \"X5: Satisfied with courier\",\n",
    "    \"X6\": \"X6: App makes ordering easy\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate([\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\"]):\n",
    "    sns.countplot(data=df, x=col, hue=\"Y\", ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {col} by Happiness\")\n",
    "    axes[i].set_xlabel(feature_explanations[col])\n",
    "    axes[i].legend(title=\"Y (Happy)\", loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "As discussed earlier,\n",
    "\n",
    "- Most people were happy about the app being easy to use (X6), paying a good price for their orders (X4) and orders being delivered on time (X1).\n",
    "- Most people were unhappy because the content of their orders were not as expected (X2).\n",
    "\n",
    "We also see that\n",
    "\n",
    "- For X5, happy customers mostly rated their courier highly but the unhappy customers were more divided, explaining the high standard deviation from earlier.\n",
    "- X3 is a bit of a mixed bag. Most people had a score of 3. If people were able to find the item that they wanted, they would be happy. Otherwise, they would be unhappy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's validate this by looking at which features happy and unhappy customers differ the most as well as the correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Y\").mean().T.assign(\n",
    "    diff=lambda x: x[1] - x[0], corr_with_Y=df.corr()[\"Y\"].drop(\"Y\")\n",
    ").sort_values(\"diff\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The biggest variance/gaps are with X1 (~0.45) and X5 (~0.52).\n",
    "\n",
    "For X1, the mean scores for the unhappy customers are still relatively high whereas for X5, the unhappy customers are leaning towards middle-low.\n",
    "\n",
    "These two features are also the ones which correlate highest with the target. That is, they are the most likely to predict happiness.\n",
    "\n",
    "On the opposite end, X2 has the highest negative correlation with Y (but still ~0). Everyone rated it poorly, which means that fixing X2 might improve overall satisfaction but not necessarily convert unhappy customer to happy customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_corr = df.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(feature_corr, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "The correlation matrix shows moderate correlations between features, especially X5 and X1 (0.43), X6 and X1 (0.41), and X5 and X3 (0.36). This suggests that some features share some overlapping information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Unhappy customers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Recall that 55% of the customers were happy and 45% of the customers were unhappy. The dataset seems well balanced.\n",
    "\n",
    "However, even if the dataset might be balanced in general, in a business persepective, it is unbalanced.\n",
    "\n",
    "45% of customers being unhappy is alarming and the company gains more from saving an unhappy customer.\n",
    "\n",
    "<br>\n",
    "\n",
    "From our analysis above, we already have a strong candidate: X2, but do we have any other features that can help us predict the truly unhappy customers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhappy_df = df[df[\"Y\"] == 0]\n",
    "happy_df = df[df[\"Y\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhappy_df.apply(lambda x: (x <= 2).sum()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Unhappy customers were most unhappy that their orders were not as expected (X2), that there was something wrong with their courier/delivery (X5) and that some items were out of stock (X3).\n",
    "\n",
    "<br>\n",
    "\n",
    "Now that we've identified what unhappy customers complain about most, let's find out which of these features actually predict unhappiness.\n",
    "\n",
    "Instead of maximizing accuracy, we should maximize **recall** for class 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: Models predict customers as happy or unhappy. If the model predicts everyone as happy we would still achieve 55% accuracy but that would be useless to us since it did not find any unhappy customers. A model that predicts everyone as unhappy would achieve 45% accuracy but then it would not flag any happy customers.\n",
    "\n",
    "Recall ensures we prioritize identifying truly unhappy customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Split Dataset and model recall for class 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We split the dataset into features and target so the model can learn: given a customer's response, predict whether they're happy or unhappy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Instead of a traditional train/test split, we use cross validation since we have a very small dataset and want to avoid overfitting.\n",
    "\n",
    "We compare recall using the following models:\n",
    "\n",
    "- Logistic Regression since it's a good linear baseline\n",
    "- Random Forest. Tree based ensemble, handles non-linear relationships\n",
    "- Gradient Boosting. Tree based ensemble\n",
    "- Support Vector Machine (SVM). Finds optimal decision boundaries, works well on small datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log_reg = cross_val_predict(LogisticRegression(), X, y)\n",
    "y_pred_rf = cross_val_predict(RandomForestClassifier(), X, y)\n",
    "y_pred_gb = cross_val_predict(GradientBoostingClassifier(), X, y)\n",
    "y_pred_svc = cross_val_predict(SVC(), X, y)  # SVC is SVM but for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(y, y_pred_log_reg, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Logistic Regression\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_rf, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Random Forest\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_gb, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Gradient Boosting\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_svc, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Support Vector Classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "For all models (class 0), the results are quite bad. Recall is below 50%.\n",
    "\n",
    "However, this is our baseline. We might be able to improve by doing a few things like hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "By default, models treat both classes equally. This means that with 55% happy and 45% unhappy, the model might lean towards predicting 'happy' more often.\n",
    "\n",
    "We can try using class_weight='balanced' to adjust weights inversely proportional to class frequencies. That is, misclassifying an unhappy customer costs more than misclassifying a happy one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log_reg = cross_val_predict(LogisticRegression(class_weight=\"balanced\"), X, y)\n",
    "y_pred_rf = cross_val_predict(RandomForestClassifier(class_weight=\"balanced\"), X, y)\n",
    "y_pred_gb = cross_val_predict(GradientBoostingClassifier(), X, y)\n",
    "y_pred_svc = cross_val_predict(SVC(class_weight=\"balanced\"), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(y, y_pred_log_reg, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Logistic Regression, balanced\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_rf, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Random Forest, balanced\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_gb, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Gradient Boosting\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_svc, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Support Vector Classifier, balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Results are better this time with SVM having the best recall for class 0 at 0.61 from 0.37. But let's see if we can do even better.\n",
    "\n",
    "What this means in practice is that we went from catching 21 out of 57 unhappy customers to catching 35 out of 57 customers.\n",
    "\n",
    "<br>\n",
    "\n",
    "Since SVM scale sensitive (uses distance to find decision boundaries), maybe we can try transforming the data with the standard scaler (removes mean and scaling to unit variance). i.e. mean=0, std=1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"svc\", SVC(class_weight=\"balanced\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc_pipe = cross_val_predict(svc_pipe, X, y)\n",
    "y_pred_log_reg = cross_val_predict(LogisticRegression(class_weight=\"balanced\"), X, y)\n",
    "y_pred_rf = cross_val_predict(RandomForestClassifier(class_weight=\"balanced\"), X, y)\n",
    "y_pred_gb = cross_val_predict(GradientBoostingClassifier(), X, y)\n",
    "y_pred_svc = cross_val_predict(SVC(class_weight=\"balanced\"), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(y, y_pred_log_reg, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Logistic Regression, balanced\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_rf, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Random Forest, balanced\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_gb, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Gradient Boosting\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_svc, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Support Vector Classifier, balanced\"\n",
    "    + \"\\n\"\n",
    "    + classification_report(y, y_pred_svc_pipe, target_names=[\"Unhappy\", \"Happy\"])\n",
    "    + \" Support Vector Classifier with standard scalar, balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "That didn't help. SVM with the standard scaler resulted in worst results than SVM just with class_weight=balanced likely because our features are already on the same 1-5 scale.\n",
    "\n",
    "Let's try something else. Using GridSearch, we might be able to find an optimal set of parameters to maximize performance. Let's also try to see what we get when we optimize for accuracy.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: To avoid data leakage (tuning and evaluating on the same dataset), we use nested cross-validation. The inner loop (GridSearchCV) tunes hyperparameters while the outer loop gives an unbiased performance estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"C\": [\n",
    "        0.01,\n",
    "        0.1,\n",
    "        1,\n",
    "        10,\n",
    "        100,\n",
    "    ],  # Regularization parameter, low C = more regularization, allows more smoother, general decision boundary\n",
    "    \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "}\n",
    "\n",
    "recall_scorer = make_scorer(recall_score, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_recall = GridSearchCV(\n",
    "    SVC(class_weight=\"balanced\"),\n",
    "    params_grid,\n",
    "    scoring=recall_scorer,\n",
    "    cv=inner_cv,\n",
    ")\n",
    "\n",
    "grid_accuracy = GridSearchCV(\n",
    "    SVC(class_weight=\"balanced\"),\n",
    "    params_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=inner_cv,\n",
    ")\n",
    "\n",
    "nested_recall = cross_val_score(grid_recall, X, y, cv=outer_cv, scoring=recall_scorer)\n",
    "nested_accuracy_with_recall_tuned = cross_val_score(\n",
    "    grid_recall, X, y, cv=outer_cv, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "nested_recall_with_accuracy_tuned = cross_val_score(\n",
    "    grid_accuracy, X, y, cv=outer_cv, scoring=recall_scorer\n",
    ")\n",
    "nested_accuracy = cross_val_score(grid_accuracy, X, y, cv=outer_cv, scoring=\"accuracy\")\n",
    "\n",
    "grid_recall.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Recall-tuned model: \\n\"\n",
    "    + f\" Recall: {nested_recall.mean():.3f}, Accuracy: {nested_accuracy_with_recall_tuned.mean():.3f} \\n\\n\"\n",
    "    + \"Accuracy-tuned model: \\n\"\n",
    "    + f\" Recall: {nested_recall_with_accuracy_tuned.mean():.3f}, Accuracy: {nested_accuracy.mean():.3f} \\n\\n\"\n",
    "    + f\"Inner CV Recall (class 0): {grid_recall.best_score_:.3f} \\n\"\n",
    "    + f\"Best parameters: {grid_recall.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The inner loop found that C=0.01 and a polynomial kernel were the best parameters.\n",
    "\n",
    "Both recall tuned and accuracy tuned models achieved ~0.57 accuracy (higher than our previous scores) but the recall tuned model had a much higher recall at 0.75.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: The nested CV score (~0.75) is different from the fixed parameter inner CV score (~0.67) because nestedCV retunes hyperparameters on each outer fold.\n",
    "\n",
    "<br>\n",
    "\n",
    "This shows that recall optimization is the superior solution. We gained performance in recall at no accuracy cost.\n",
    "\n",
    "This 0.75 recall is the best estimate of how the model will perform on new unseen data.\n",
    "\n",
    "<br>\n",
    "\n",
    "For completion, let's verify that 0.57 accuracy is the best we can get with the four models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(class_weight=\"balanced\", max_iter=1000),\n",
    "        \"params\": {\n",
    "            \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "            \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "        },\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [3, 5, 10, None],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "        },\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [2, 3, 5],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        },\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(class_weight=\"balanced\"),\n",
    "        \"params\": {\n",
    "            \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "            \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    grid = GridSearchCV(\n",
    "        config[\"model\"],\n",
    "        config[\"params\"],\n",
    "        scoring=\"accuracy\",\n",
    "        cv=inner_cv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    nested_scores = cross_val_score(grid, X, y, cv=outer_cv, scoring=\"accuracy\")\n",
    "\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": nested_scores.mean(),\n",
    "            \"Std\": nested_scores.std(),\n",
    "            \"Best Params\": grid.best_params_,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {nested_scores.mean():.3f} ± {nested_scores.std():.3f}\")\n",
    "    print(f\"  Best params: {grid.best_params_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "The best result was Gradient Boosting at ~0.63 accuracy with all four tuned models being in the ~0.57 to ~0.63 range.\n",
    "\n",
    "This suggests that despite tuning across multiple models, we will not be able to achieve a 73% accuracy score and that there is a limitation in the dataset (e.g. small sample size, weak feature to target correlation etc.).\n",
    "\n",
    "Therefore further optimizing for accuracy does not make sense and as previously stated, recall optimization is the way to go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Now let's answer the bonus question: which features actually matter for predicting unhappiness? And can we remove any unnecessary ones?\n",
    "\n",
    "By identifying the minimal feature set on our best model (tuned SVM), we can simplify future surveys while keeping the predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_svm = SVC(C=0.01, kernel=\"poly\", class_weight=\"balanced\")\n",
    "tuned_svm.fit(X, y)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    tuned_svm, X, y, scoring=recall_scorer, n_repeats=30, random_state=42\n",
    ")\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X.columns, \"Importance\": perm.importances_mean}\n",
    ").sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "importance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "We see that X1 contributed the most followed by X2, X6, X3, X4 then X5 but importance does not equate to necessity. Can we figure out which ones are really necessary?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: Recall from the EDA section that X5 had the largest mean difference between unhappy and happy customers. In the permutation importance results, we see that X5 is ranked the least important. This makes sense since X5 correlates with X1 and X3 (see correlation matrix above), and is deemed 'redundant'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Feature Engineering - Minimal Feature Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Recursive feature elimination with cross-validation (RFECV) on SVM requires us to use a kernel other than 'poly'. This does not give us the result we want so we have to take another approach.\n",
    "\n",
    "Let's try dropping each feature and seeing how recall changes. We first use cross validation with the optimal fixed hyperparameters to find the minimal set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\"]\n",
    "\n",
    "scores = cross_val_score(\n",
    "    tuned_svm, X[all_features], y, cv=outer_cv, scoring=recall_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline (all 6): \\n\" + f\" Recall = {scores.mean():.3f} ± {scores.std():.3f}\\n\")\n",
    "\n",
    "print(\"Drop one feature:\")\n",
    "for drop in all_features:\n",
    "    features = [f for f in all_features if f != drop]\n",
    "    scores = cross_val_score(\n",
    "        tuned_svm, X[features], y, cv=outer_cv, scoring=recall_scorer\n",
    "    )\n",
    "    print(f\" Drop {drop}: Recall = {scores.mean():.3f} ± {scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "We see that\n",
    "\n",
    "- Dropping X1 hurts our score (=0.58) while dropping X2 increases the score (=~0.69)\n",
    "- Dropping X4 and X5 does not affect the model much (=0.65)\n",
    "- Dropping X3 and X6 results in a lower score (=0.63), maybe a bit too low for our liking.\n",
    "\n",
    "Let's test combinations that drop X2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = {\n",
    "    \"Drop X2\": [\"X1\", \"X3\", \"X4\", \"X5\", \"X6\"],\n",
    "    \"Drop X2, X4\": [\"X1\", \"X3\", \"X5\", \"X6\"],\n",
    "    \"Drop X2, X5\": [\"X1\", \"X3\", \"X4\", \"X6\"],\n",
    "    \"Drop X2, X4, X5\": [\"X1\", \"X3\", \"X6\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Promising combinations:\")\n",
    "for name, features in test_sets.items():\n",
    "    scores = cross_val_score(\n",
    "        tuned_svm, X[features], y, cv=outer_cv, scoring=recall_scorer\n",
    "    )\n",
    "    print(f\"{name}: Recall = {scores.mean():.3f} ± {scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Dropping X2, X4 and X5 improves recall from ~0.67 to ~0.74.\n",
    "\n",
    "Let's validate this with the nested CV model (gridsearch). The recall for this model with 6 features was ~0.75.\n",
    "\n",
    "If we can get ~0.75 with the 3 feature set, then we can confirm that the minimal feature set is [X1, X3, X6].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_reduced = cross_val_score(\n",
    "    grid_recall, X[[\"X1\", \"X3\", \"X6\"]], y, cv=outer_cv, scoring=recall_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Nested CV (3 features): {nested_reduced.mean():.3f} ± {nested_reduced.std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "The result is now clear. Our minimal feature set is X1, X3, and X6.\n",
    "\n",
    "We can remove X2, X4 and X5 from future surveys, which reduces survey length by 50% without reducing model performance for identifying unhappy customers.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: The high standard deviation is a reflection of the small sample size:\n",
    "\n",
    "- We used a StratifiedKfold of 5 splits. So 126 samples / 5 = ~25 samples per test fold.\n",
    "- ~45% are unhappy so 25\\*0.45 = ~ 11 unhappy customer per fold\n",
    "- Identifying one more/less correct = ~9% swing in recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "**Goal**: Predict customer happiness or unhappiness with 73% accuracy or justify a superior solution.\n",
    "\n",
    "**Approach**: Focus on identifying unhappy customers (recall for class 0) instead of overall accuracy because this has a higher business value as 45% of customers are dissatisfied.\n",
    "\n",
    "- Our tuned SVM with nested cross validation (optimal parameters: C=0.01, polynomial kernel) achieved 75.2% recall for unhappy customers. That means we can identify 3 out of 4 unsatisfied customers.\n",
    "\n",
    "- All four tuned models had an accuracy score in ~57-63% range, which shows that accuracy plateaus at ~63% regardless of optimization but the recall-tuned SVM model also achieved 75% recall vs only 60% for the accuracy tuned SVM model, thereby identifying 15% more unhappy customers at no accuracy cost.\n",
    "\n",
    "- The top complaints from the customer survey were X2 (contents of my order was as I expected) and X5 (I am satisfied with my courier). It's a universal pain point and adds noise to the model.\n",
    "\n",
    "- Removing features X2, X4 (I paid a good price for my order) and X5 from the survey resulted in comparable model performance (from 0.752 to 0.753). That is, we reduced survey length by 50% while keeping model performance by going from a 6 feature set to a 3 feature set.\n",
    "\n",
    "**Recommendations**:\n",
    "\n",
    "- Prioritize X1, X3, X6 or\n",
    "- Reduce survey length from 6 questions to 3 questions (X1, X3, X6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
